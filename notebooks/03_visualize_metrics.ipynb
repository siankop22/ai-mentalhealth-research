{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26beb955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: /Users/tksiankop/Desktop/ai-mentalhealth-research/notebooks\n",
      "Using metrics dir: /Users/tksiankop/Desktop/ai-mentalhealth-research/reports/experiments\n"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"CWD:\", Path.cwd())\n",
    "\n",
    "# Try both possibilities: running from project root vs from /notebooks\n",
    "candidates = [Path(\"reports/experiments\"), Path(\"../reports/experiments\")]\n",
    "metrics_dir = next((p for p in candidates if p.exists()), None)\n",
    "\n",
    "if metrics_dir is None:\n",
    "    # Show what actually exists to debug fast\n",
    "    print(\"Existing reports dirs:\", [p for p in [Path(\"reports\"), Path(\"../reports\")] if p.exists()])\n",
    "    raise FileNotFoundError(\"Could not find reports/experiments from current CWD.\")\n",
    "\n",
    "print(\"Using metrics dir:\", metrics_dir.resolve())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4746b149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found files: ['xlmr_metrics.json', 'xlmr_metrics_20251005_025500.json']\n",
      "Parsed rows: 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>class</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "      <th>eval_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xlmr_metrics.json</td>\n",
       "      <td>distress</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.714077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xlmr_metrics.json</td>\n",
       "      <td>macro avg</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.714077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xlmr_metrics.json</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.714077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>xlmr_metrics.json</td>\n",
       "      <td>weighted avg</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.228571</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.714077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xlmr_metrics_20251005_025500.json</td>\n",
       "      <td>distress</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.714077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>xlmr_metrics_20251005_025500.json</td>\n",
       "      <td>macro avg</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.714077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>xlmr_metrics_20251005_025500.json</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.714077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>xlmr_metrics_20251005_025500.json</td>\n",
       "      <td>weighted avg</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.228571</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.714077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                file         class  precision  recall  \\\n",
       "0                  xlmr_metrics.json      distress       0.00     0.0   \n",
       "1                  xlmr_metrics.json     macro avg       0.20     0.5   \n",
       "2                  xlmr_metrics.json       neutral       0.40     1.0   \n",
       "3                  xlmr_metrics.json  weighted avg       0.16     0.4   \n",
       "4  xlmr_metrics_20251005_025500.json      distress       0.00     0.0   \n",
       "5  xlmr_metrics_20251005_025500.json     macro avg       0.20     0.5   \n",
       "6  xlmr_metrics_20251005_025500.json       neutral       0.40     1.0   \n",
       "7  xlmr_metrics_20251005_025500.json  weighted avg       0.16     0.4   \n",
       "\n",
       "   f1-score  support  eval_loss  \n",
       "0  0.000000      3.0   0.714077  \n",
       "1  0.285714      5.0   0.714077  \n",
       "2  0.571429      2.0   0.714077  \n",
       "3  0.228571      5.0   0.714077  \n",
       "4  0.000000      3.0   0.714077  \n",
       "5  0.285714      5.0   0.714077  \n",
       "6  0.571429      2.0   0.714077  \n",
       "7  0.228571      5.0   0.714077  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Pick the right directory whether the notebook runs from root or /notebooks\n",
    "candidates = [Path(\"reports/experiments\"), Path(\"../reports/experiments\")]\n",
    "metrics_dir = next((p for p in candidates if p.exists()), None)\n",
    "assert metrics_dir is not None, \"Could not find reports/experiments/\"\n",
    "\n",
    "paths = sorted(metrics_dir.glob(\"xlmr_metrics*.json\"))\n",
    "print(\"Found files:\", [p.name for p in paths])\n",
    "\n",
    "rows = []\n",
    "for p in paths:\n",
    "    try:\n",
    "        with open(p, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Skipping {p.name}: not valid JSON ({e})\")\n",
    "        continue\n",
    "\n",
    "    # Handle multiple possible shapes\n",
    "    if isinstance(data, (int, float)):\n",
    "        # a file that just contains a number (treat as eval_loss only)\n",
    "        rows.append({\n",
    "            \"file\": p.name, \"class\": \"N/A\",\n",
    "            \"precision\": None, \"recall\": None, \"f1-score\": None, \"support\": None,\n",
    "            \"eval_loss\": float(data)\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    if not isinstance(data, dict):\n",
    "        print(f\"[WARN] Skipping {p.name}: unexpected top-level type {type(data)}\")\n",
    "        continue\n",
    "\n",
    "    eval_loss = data.get(\"eval_loss\")\n",
    "    report = data.get(\"report\")\n",
    "\n",
    "    # If report missing or not a dict, still record the loss row\n",
    "    if not isinstance(report, dict):\n",
    "        rows.append({\n",
    "            \"file\": p.name, \"class\": \"N/A\",\n",
    "            \"precision\": None, \"recall\": None, \"f1-score\": None, \"support\": None,\n",
    "            \"eval_loss\": eval_loss\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    # Normal case: per-class stats inside 'report'\n",
    "    for klass, stats in report.items():\n",
    "        # each stats should be a dict with keys: precision, recall, f1-score, support\n",
    "        if isinstance(stats, dict):\n",
    "            rows.append({\n",
    "                \"file\": p.name,\n",
    "                \"class\": klass,\n",
    "                \"precision\": stats.get(\"precision\"),\n",
    "                \"recall\": stats.get(\"recall\"),\n",
    "                \"f1-score\": stats.get(\"f1-score\"),\n",
    "                \"support\": stats.get(\"support\"),\n",
    "                \"eval_loss\": eval_loss,\n",
    "            })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print(\"Parsed rows:\", len(df))\n",
    "\n",
    "# Show main table (neutral/distress only if present)\n",
    "if not df.empty:\n",
    "    display(\n",
    "        df[df[\"class\"].isin([\"neutral\",\"distress\",\"macro avg\",\"weighted avg\",\"N/A\"])]\n",
    "        .sort_values([\"file\",\"class\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "else:\n",
    "    print(\"No usable metrics found.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
