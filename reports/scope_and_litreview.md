ðŸ§­ Research Scope (1 page)

Title:
Detecting Distress in Multilingual Text: Building an AI System for Early Mental Health Support Across English and Burmese/Zomi

Problem Statement:
Millions share mental health struggles online, but most tools focus on English-only data and miss expressions in low-resource languages like Burmese or Zomi. This project aims to build and evaluate a multilingual model that identifies early signs of emotional distress in short text posts while minimizing false positives. The goal is not diagnosis but timely supportâ€”helping bridge the language and access gap in digital mental health monitoring.

Objectives:

Collect or adapt small Burmese/Zomi text samples reflecting emotional tone.

Train baseline (TF-IDF + Logistic Regression) and multilingual transformer (XLM-R) models.

Evaluate cross-lingual performance and robustness.

Produce an ethical, reproducible prototype and data/model cards.

Target Metrics:

Primary: Macro-F1 â‰¥ 0.75 on distress detection.

Secondary: Recall â‰¥ 0.85 for the distress class; < 10% variance between English vs. Burmese/Zomi F1.

Runtime goal: < 300 ms inference on CPU.

Ethics Guardrails:

The system does not replace clinicians or provide diagnostic claims.

All data will be public, anonymized, or synthetic.

Sensitive posts will be paraphrased or excluded.

Model outputs include a clear disclaimer and resource links (e.g., hotlines).

Results and biases (language, cultural expression) will be transparently reported.

ðŸ“š Seed Papers and Summaries
Milne, D. N., Pink, G., & Calvo, R. A. (2016). â€œDetecting Depression and Anxiety from Social Media Text.â€ IEEE Transactions on Affective Computing.	2016	Milne et al. present early work using lexical and linguistic cues from Twitter and forums to classify depression and anxiety. They compare n-gram and LIWC features with SVM and Random Forest classifiers, achieving moderate F1 (â‰ˆ 0.70). The paperâ€™s strength lies in its careful annotation scheme and analysis of language patterns. However, itâ€™s limited to English, short posts, and small data diversity. It highlights how â€œinformal emotional cuesâ€â€”misspellings, emojis, and metaphorsâ€”are strong indicators of mood. For this project, the paper provides baseline features and motivates inclusion of informal markers in multilingual text.
Suhaimi, N. S., & Sani, N. (2020). â€œCross-Lingual Sentiment Analysis for Low-Resource Languages Using Multilingual BERT.â€ Procedia Computer Science.	2020	This paper explores adapting Multilingual BERT (mBERT) for Malay and Indonesian sentiment classification via zero-shot transfer from English. The authors fine-tune on English datasets, then test on translated text and small native samples. mBERT maintains 70â€“80% of English accuracy, showing promising transfer even with minimal data. Limitations include poor idiom coverage and cultural nuance loss. The work supports the idea that transformer-based multilingual encoders can generalize across languages sharing limited data. This informs the proposed projectâ€™s use of XLM-R for Burmese/Zomi by leveraging English mental-health corpora and small native samples.
Tadesse, M. M., et al. (2019). â€œDetection of Depression-Related Posts on Reddit Using Machine Learning Techniques.â€ PeerJ Computer Science.	2019	Tadesse et al. curate a large Reddit dataset labeled for depression and anxiety, applying TF-IDF, Word2Vec, and LSTM models. Their best model (Bi-LSTM) achieves F1 â‰ˆ 0.78. They also emphasize ethical risks of misclassification and privacy. Importantly, they provide a transparent preprocessing pipeline and open data, enabling reproducibility. This serves as a solid English benchmark dataset for this projectâ€™s initial training. The methodologyâ€”feature extraction, model comparison, and error analysisâ€”provides a template for evaluating multilingual extensions while maintaining ethical data handling practices.